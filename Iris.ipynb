{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for the implementation of task IRIS\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For computation\n",
    "import numpy as np\n",
    "# For confusion matrices\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Importing labeled data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, instances):\n",
    "        data = {\"training\": {\"targets\": [], \"features\":[]}, \"testing\": {\"targets\": [], \"features\":[]}}\n",
    "        labelToTarget = {\"Iris-setosa\": [1, 0, 0], \"Iris-versicolor\": [0, 1, 0], \"Iris-virginica\": [0, 0, 1]}\n",
    "        for instance in instances:\n",
    "            match instance.set:\n",
    "                case 'training':\n",
    "                    data[\"training\"][\"targets\"].append(labelToTarget[instance.label])\n",
    "                    data[\"training\"][\"features\"].append(instance.features)\n",
    "                case 'testing':\n",
    "                    data[\"testing\"][\"targets\"].append(labelToTarget[instance.label])\n",
    "                    data[\"testing\"][\"features\"].append(instance.features)\n",
    "                    \n",
    "        # convert to numpy array\n",
    "        data[\"training\"][\"targets\"] = np.array(data[\"training\"][\"targets\"]).astype(float)\n",
    "        data[\"training\"][\"features\"] = np.array(data[\"training\"][\"features\"]).astype(float)\n",
    "        data[\"testing\"][\"targets\"] = np.array(data[\"testing\"][\"targets\"]).astype(float)\n",
    "        data[\"testing\"][\"features\"]  = np.array(data[\"testing\"][\"features\"]).astype(float)\n",
    "        \n",
    "        self.data = data\n",
    "        self.classes_names = [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"]\n",
    "        self.feature_names = ['Sepal length [cm]', 'Sepal width [cm]', 'Petal length [cm]', 'Petal width [cm]']\n",
    "        self.DESCR = \"Iris dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Instance:\n",
    "    def __init__(self, features, label, set):\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self.set = set    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet(features_list: list):\n",
    "    Instances = []\n",
    "    path = \"Iris_TTT4275/iris.data\"\n",
    "    n_classes = 3\n",
    "    n_training = 30\n",
    "    n_testing  = 20\n",
    "    with open(path) as file:\n",
    "        for _ in range(n_classes):\n",
    "            for _ in range(n_training):\n",
    "                line = file.readline()\n",
    "                line = line.split(',')\n",
    "                features = []\n",
    "                for i in features_list:\n",
    "                    features.append(line[i]) \n",
    "                label = line[-1].strip(\"\\n\")\n",
    "                training_instance = Instance(features=features, label=label, set=\"training\")\n",
    "                Instances.append(training_instance)\n",
    "            for _ in range(n_testing):\n",
    "                line = file.readline()\n",
    "                line = line.split(',')\n",
    "                features = []\n",
    "                for i in features_list:\n",
    "                    features.append(line[i]) \n",
    "                label = line[-1].strip(\"\\n\")\n",
    "                testing_instance = Instance(features=features, label=label, set=\"testing\")\n",
    "                Instances.append(testing_instance)\n",
    "    return Dataset(Instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS_Dataset = loadDataSet([0, 1, 2, 3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Training a linear classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear classifier equations\n",
    "\n",
    "from the [Compendium-Part-III-Classification](Resources/Compendium_Classification.pdf)\n",
    "\n",
    "##### Chapter 2.4 \n",
    "\n",
    "$$\n",
    "x \\in \\omega_j \\iff g_j(x) = \\max_i g_i(x)\n",
    "$$\n",
    "x is in the class ($\\omega_j$) that has the highest value for the discriminant function ($g_j$). This is equivalent to making a partition of the whole input space, and for each point you assign the point to the class with the highest \"probability\" that the point belongs to that class.\n",
    "The discrimination function:\n",
    "$$\n",
    "g_i(x) = \\omega_i^T x + \\omega_{io} , \\qquad i = i, \\dots, C \n",
    "$$\n",
    "Where $\\omega_i$ is the class $x$ is the feature vector, and the $\\omega_{io}$ is the offset for the class i. In our case C =3, so we can write the disctimination function in compact form: \n",
    "$$\n",
    "g = W x + \\omega_o \n",
    "$$\n",
    "where $W \\in M_{CxF}(\\mathbb{R})$ and $g$ and $\\omega_o$ both are column vectors with size C (#classes). F stands for #features. In our case:  $C=3 \\land F=4$    \n",
    "When training we are aiming at finding the best values for $W$ and $\\omega_o$\n",
    "\n",
    "##### Chapter 3.2 \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get only one linear term, we do one more trick:\n",
    "This trick yields the same equations only that it now gets more compact once again\n",
    "$$\n",
    "\\begin{bmatrix} W & \\omega_o \\end{bmatrix} \\to W\\\\\n",
    "\\begin{bmatrix} x^T & 1 \\end{bmatrix} \\to x \\\\\n",
    "\\implies g = Wx\n",
    "$$\n",
    "Loss function\n",
    "$$\n",
    "MSE = \\frac{1}{2} \\sum_{k=1}^{N}(g_k-t_k)^T(g_k-t_k)\n",
    "$$\n",
    "To decide how good our model is doing. Here t is the target vector (correct labeled features for the training set.)\n",
    "Activation function:\n",
    "$$\n",
    "g_{k} = \\frac{1}{1+\\exp^{-z_{ik}}}, \\qquad z_{ik} = Wx_k \\\\\n",
    "$$\n",
    "We should ideally use a heavyside function, but we need the derivative, and therefor we us the function up above (sigmoid function). This is used for the discrimination function.\n",
    "\n",
    "The way to go? We want to minimize the MSE, based on choosing the W matrix. We use gradient descent.\n",
    "Then we need to hyperparameters which we are tuning:\n",
    "$$\n",
    "\\alpha: \\text{Learning rate [0.0-1.0]}\\\\\n",
    "n: \\text{number of epochs (how many times we improve the W-matrix [0<])}\n",
    "$$\n",
    "Based on them we are using the gradient descent:\n",
    "$$\n",
    "\\text{for i in range(1, n+1):}\\\\\n",
    " \\qquad  \\qquad W_{i+1} = W_{i} - \\alpha \\nabla_WMSE\n",
    "$$\n",
    "For the calculation of the gradient we once again have calculations from the [Compendium-Part-III-Classification](Resources/Compendium_Classification.pdf)\n",
    "\n",
    "Using the chain rule\n",
    "$$\n",
    "\\nabla_W MSE = \\sum^{N}_{k=1}\\nabla_{g_{k}} \\, MSE \\, \\nabla_{z_{k}} \\, g_k \\, \\nabla_W z_k\\\\\n",
    "$$\n",
    "Some simplifications\n",
    "$$\n",
    "\\nabla_{g_k}MSE = g_k-t_k\\\\\n",
    "\\nabla_{z_k}g = g_k \\circ (1-g_k)\\\\\n",
    "\\nabla_Wz_k = x_k^T\\\\\n",
    "$$\n",
    "Yields the result\n",
    "$$\n",
    "\\nabla_W MSE = \\sum^{N}_{k=1} [(g_k-t_k) \\circ g_k \\circ (1-g_k)]x_k^T\n",
    "$$\n",
    "Where $\\circ$ is elementwise multiplication."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainClassifier(dataset: Dataset, step_length: float, n_iterations, W_0: np.ndarray):\n",
    "    W = W_0\n",
    "    MSEs = []\n",
    "    n_classes = 3\n",
    "    n_training = 30\n",
    "    for i in range(n_iterations):\n",
    "        Gradient_MSE = np.zeros(W_0.shape)\n",
    "        MSE = 0\n",
    "        for index in range(n_classes * n_training):\n",
    "            x_k = dataset.data[\"training\"][\"features\"][index]\n",
    "            x_k = np.append(x_k, 1)\n",
    "            z_k = np.dot(W, x_k)\n",
    "            g_k = sigmoid(z_k)\n",
    "            t_k = dataset.data[\"training\"][\"targets\"][index]\n",
    "            MSE += 1/2 * np.dot((g_k - t_k).T,(g_k - t_k))\n",
    "            Gradient_MSE += np.outer((g_k - t_k)*g_k*(np.ones((1,3))-g_k), x_k.T)\n",
    "        W = W - step_length * Gradient_MSE\n",
    "        MSEs.append(MSE)\n",
    "    print(f\"MSE in first iteration {MSEs[0]}, and last iteration {MSEs[-1]}\\n\")\n",
    "    print(f\"Our W is \\n{W}\\n\")\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hyperparameters, step length and number of epochs\n",
    "'''\n",
    "alpha = 0.01\n",
    "n = 10000\n",
    "W_0 = np.zeros((3, 5))\n",
    "W = TrainClassifier(IRIS_Dataset, alpha, n, W_0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing part"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestClassifier(dataset: Dataset, W: np.ndarray):\n",
    "    n_classes = 3\n",
    "    n_training = 30\n",
    "    n_testing = 20\n",
    "    ConfMatrices = {\"training\": np.zeros((n_classes, n_classes)), \"testing\": np.zeros((n_classes, n_classes))}\n",
    "    errors = {\"training\": 0, \"testing\": 0}\n",
    "    for index in range(n_classes * n_testing):\n",
    "        x_k = IRIS_Dataset.data[\"testing\"][\"features\"][index]\n",
    "        x_k = np.append(x_k, 1)\n",
    "        t_k = IRIS_Dataset.data[\"testing\"][\"targets\"][index]\n",
    "        g_k = np.dot(W, x_k)\n",
    "        ConfMatrices[\"testing\"][np.argmax(t_k), np.argmax(g_k)] += 1\n",
    "        if np.argmax(t_k) != np.argmax(g_k):\n",
    "            errors[\"testing\"] += 1\n",
    "    \n",
    "    for index in range(n_classes * n_training):\n",
    "        x_k = IRIS_Dataset.data[\"training\"][\"features\"][index]\n",
    "        x_k = np.append(x_k, 1)\n",
    "        t_k = IRIS_Dataset.data[\"training\"][\"targets\"][index]\n",
    "        g_k = np.dot(W, x_k)\n",
    "        ConfMatrices[\"training\"][np.argmax(t_k), np.argmax(g_k)] += 1\n",
    "        if np.argmax(t_k) != np.argmax(g_k):\n",
    "            errors[\"training\"] += 1\n",
    "    \n",
    "    print(f\"Error-rates: \\nTraining: {errors['training']/(n_training*n_classes)}, Testing: {errors['testing']/(n_classes*n_testing)}\\n\")\n",
    "    return ConfMatrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotConfusionMatrix(matrix):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16,8))\n",
    "    ax[0].set_title(\"Training\")\n",
    "    ax[1].set_title(\"Testing\")\n",
    "    metrics.ConfusionMatrixDisplay(confusion_matrix=matrix[\"training\"], \n",
    "                                   display_labels=IRIS_Dataset.classes_names,\n",
    "                                   ).plot(ax=ax[0], cmap=\"Blues\")\n",
    "    metrics.ConfusionMatrixDisplay(confusion_matrix=matrix[\"testing\"],\n",
    "                                   display_labels=IRIS_Dataset.classes_names,\n",
    "                                   ).plot(ax=ax[1], cmap=\"Blues\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfMatrix = TestClassifier(IRIS_Dataset, W)\n",
    "\n",
    "PlotConfusionMatrix(ConfMatrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Iris_TTT4275/iris.data\"\n",
    "n_classes = 3\n",
    "n_training = 20\n",
    "n_testing  = 30\n",
    "\n",
    "Instances = []\n",
    "\n",
    "with open(path) as file:\n",
    "    for _ in range(n_classes):\n",
    "        for _ in range(n_testing):\n",
    "            line = file.readline()\n",
    "            line = line.split(',')\n",
    "            features = line[0:-1]\n",
    "            label = line[-1].strip(\"\\n\")\n",
    "            testing_instance = Instance(features=features, label=label, set=\"testing\")\n",
    "            Instances.append(testing_instance)\n",
    "            \n",
    "        for _ in range(n_training):\n",
    "            line = file.readline()\n",
    "            line = line.split(',')\n",
    "            features = line[0:-1]\n",
    "            label = line[-1].strip(\"\\n\")\n",
    "            training_instance = Instance(features=features, label=label, set=\"training\")\n",
    "            Instances.append(training_instance)\n",
    "IRIS_Dataset = Dataset(Instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hyperparameters, step length and number of epochs\n",
    "'''\n",
    "alpha = 0.01\n",
    "n = 10000\n",
    "W_0 = np.zeros((3, 5))\n",
    "\n",
    "W = W_0\n",
    "MSEs = []\n",
    "n_classes = 3\n",
    "n_training = 20\n",
    "for i in range(n):\n",
    "    Gradient_MSE = np.zeros(W_0.shape)\n",
    "    MSE = 0\n",
    "    for index in range(n_classes * n_training):\n",
    "        x_k = IRIS_Dataset.data[\"training\"][\"features\"][index]\n",
    "        x_k = np.append(x_k, 1)\n",
    "        z_k = np.dot(W, x_k)\n",
    "        g_k = sigmoid(z_k)\n",
    "        t_k = IRIS_Dataset.data[\"training\"][\"targets\"][index]\n",
    "        MSE += 1/2 * np.dot((g_k - t_k).T,(g_k - t_k))\n",
    "        Gradient_MSE += np.outer((g_k - t_k)*g_k*(np.ones((1,3))-g_k), x_k.T)\n",
    "    W = W - alpha * Gradient_MSE\n",
    "    MSEs.append(MSE)\n",
    "print(f\"MSE in first iteration {MSEs[0]}, and last iteration {MSEs[-1]}\\n\")\n",
    "print(f\"Our W is {W}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 3\n",
    "n_training = 20\n",
    "n_testing = 30\n",
    "ConfMatrix = {\"training\": np.zeros((n_classes, n_classes)), \"testing\": np.zeros((n_classes, n_classes))}\n",
    "for index in range(n_classes * n_testing):\n",
    "    x_k = IRIS_Dataset.data[\"testing\"][\"features\"][index]\n",
    "    x_k = np.append(x_k, 1)\n",
    "    t_k = IRIS_Dataset.data[\"testing\"][\"targets\"][index]\n",
    "    g_k = np.dot(W, x_k)\n",
    "    ConfMatrix[\"testing\"][np.argmax(t_k), np.argmax(g_k)] += 1\n",
    "\n",
    "for index in range(n_classes * n_training):\n",
    "    x_k = IRIS_Dataset.data[\"training\"][\"features\"][index]\n",
    "    x_k = np.append(x_k, 1)\n",
    "    t_k = IRIS_Dataset.data[\"training\"][\"targets\"][index]\n",
    "    g_k = np.dot(W, x_k)\n",
    "    ConfMatrix[\"training\"][np.argmax(t_k), np.argmax(g_k)] += 1\n",
    "\n",
    "PlotConfusionMatrix(ConfMatrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO: Set dei ved siden av kvarandre. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f in range(4):\n",
    "#     for g in range(4):\n",
    "#         if f != g:\n",
    "#             plt.figure()\n",
    "#             plt.scatter(IRIS_Dataset.data[\"training\"][\"features\"][:,f], IRIS_Dataset.data[\"training\"][\"features\"][:,g], c=IRIS_Dataset.data[\"training\"][\"targets\"][:,0])\n",
    "#             plt.title(f\"Feature {f} vs feature {g}\")\n",
    "#             plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 50\n",
    "paths = [\"Iris_TTT4275/class_1\", \"Iris_TTT4275/class_2\", \"Iris_TTT4275/class_3\"]\n",
    "features = {paths[0]: [[], [], [], []], paths[1]:[[], [], [], []], paths[2]:[[], [], [], []]}\n",
    "for i in range(len(paths)):\n",
    "    with open(paths[i]) as file:\n",
    "        for j in range(n_examples):\n",
    "            line = file.readline().strip('\\n').split(',')\n",
    "            for k in range(len(line)):\n",
    "                features[paths[i]][k].append(float(line[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bars = 6\n",
    "opacity = 0.5\n",
    "plt.figure(figsize=(13,13))\n",
    "for index, feature in enumerate(IRIS_Dataset.feature_names):\n",
    "    plt.subplot(2, 2, index+1)\n",
    "    for class_nr in range(n_classes):\n",
    "        plt.hist(features[paths[class_nr]][index], label=IRIS_Dataset.classes_names[class_nr], bins=n_bars, alpha=opacity)\n",
    "    plt.legend()\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('count')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Can be seen by the plots, feature 'Sepal width' is caotic and should be excluded. This is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS_Dataset = loadDataSet([0, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hyperparameters, step length and number of epochs\n",
    "'''\n",
    "alpha = 0.01\n",
    "n = 10000\n",
    "W_0 = np.zeros((3, 4))\n",
    "\n",
    "W = TrainClassifier(IRIS_Dataset, alpha, n, W_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfMatrix = TestClassifier(IRIS_Dataset, W)\n",
    "\n",
    "PlotConfusionMatrix(ConfMatrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With only two features:  Petal width & length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS_Dataset = loadDataSet([2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hyperparameters, step length and number of epochs\n",
    "'''\n",
    "alpha = 0.01\n",
    "n = 10000\n",
    "W_0 = np.zeros((3, 3))\n",
    "\n",
    "W = TrainClassifier(IRIS_Dataset, alpha, n, W_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfMatrix = TestClassifier(IRIS_Dataset, W)\n",
    "\n",
    "PlotConfusionMatrix(ConfMatrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And with only one feature (Petal width):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS_Dataset = loadDataSet([3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hyperparameters, step length and number of epochs\n",
    "'''\n",
    "alpha = 0.01\n",
    "n = 10000\n",
    "W_0 = np.zeros((3, 2))\n",
    "\n",
    "W = TrainClassifier(IRIS_Dataset, alpha, n, W_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfMatrix = TestClassifier(IRIS_Dataset, W)\n",
    "\n",
    "PlotConfusionMatrix(ConfMatrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "* Comment on linear separability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
