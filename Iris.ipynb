{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for the implementation of task IRIS\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For computation\n",
    "import numpy as np\n",
    "# For confusion matrices and plotting\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining classes and loading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, instances):\n",
    "        data = {\"training\": {\"targets\": [], \"features\":[]}, \"testing\": {\"targets\": [], \"features\":[]}}\n",
    "        labelToTarget = {\"Iris-setosa\": [1, 0, 0], \"Iris-versicolor\": [0, 1, 0], \"Iris-virginica\": [0, 0, 1]}\n",
    "        for instance in instances:\n",
    "            match instance.set:\n",
    "                case 'training':\n",
    "                    data[\"training\"][\"targets\"].append(labelToTarget[instance.label])\n",
    "                    data[\"training\"][\"features\"].append(instance.features)\n",
    "                case 'testing':\n",
    "                    data[\"testing\"][\"targets\"].append(labelToTarget[instance.label])\n",
    "                    data[\"testing\"][\"features\"].append(instance.features)\n",
    "                    \n",
    "        # convert to numpy array\n",
    "        data[\"training\"][\"targets\"] = np.array(data[\"training\"][\"targets\"]).astype(float)\n",
    "        data[\"training\"][\"features\"] = np.array(data[\"training\"][\"features\"]).astype(float)\n",
    "        data[\"testing\"][\"targets\"] = np.array(data[\"testing\"][\"targets\"]).astype(float)\n",
    "        data[\"testing\"][\"features\"]  = np.array(data[\"testing\"][\"features\"]).astype(float)\n",
    "        \n",
    "        self.data = data\n",
    "        self.classes_names = [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"]\n",
    "        self.feature_names = ['Sepal length [cm]', 'Sepal width [cm]', 'Petal length [cm]', 'Petal width [cm]']\n",
    "        self.colors = ['red', 'green', 'blue']\n",
    "        self.DESCR = \"Iris plants dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Instance:\n",
    "    def __init__(self, features, label, set):\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self.set = set    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet(features_list: list):\n",
    "    Instances = []\n",
    "    path = \"Iris_TTT4275/iris.data\"\n",
    "    n_classes = 3\n",
    "    n_training = 30\n",
    "    n_testing  = 20\n",
    "    with open(path) as file:\n",
    "        for _ in range(n_classes):\n",
    "            for _ in range(n_training):\n",
    "                line = file.readline()\n",
    "                line = line.split(',')\n",
    "                features = []\n",
    "                for i in features_list:\n",
    "                    features.append(line[i]) \n",
    "                label = line[-1].strip(\"\\n\")\n",
    "                training_instance = Instance(features=features, label=label, set=\"training\")\n",
    "                Instances.append(training_instance)\n",
    "            for _ in range(n_testing):\n",
    "                line = file.readline()\n",
    "                line = line.split(',')\n",
    "                features = []\n",
    "                for i in features_list:\n",
    "                    features.append(line[i]) \n",
    "                label = line[-1].strip(\"\\n\")\n",
    "                testing_instance = Instance(features=features, label=label, set=\"testing\")\n",
    "                Instances.append(testing_instance)\n",
    "    return Dataset(Instances)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the iris-dataset with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS_Dataset = loadDataSet([0, 1, 2, 3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Training a linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):                 # Activation function\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainClassifier(dataset: Dataset, step_length: float, n_iterations, W_0: np.ndarray):\n",
    "    W = W_0\n",
    "    MSEs = []\n",
    "    n_classes = 3\n",
    "    n_training = 30\n",
    "    for _ in range(n_iterations):\n",
    "        Gradient_MSE = np.zeros(W_0.shape)\n",
    "        MSE = 0\n",
    "        for index in range(n_classes * n_training):\n",
    "            x_k = dataset.data[\"training\"][\"features\"][index]\n",
    "            x_k = np.append(x_k, 1)\n",
    "            z_k = np.dot(W, x_k)\n",
    "            g_k = sigmoid(z_k)\n",
    "            t_k = dataset.data[\"training\"][\"targets\"][index]\n",
    "            MSE += 1/2 * np.dot((g_k - t_k).T,(g_k - t_k))\n",
    "            Gradient_MSE += np.outer((g_k - t_k)*g_k*(np.ones((1,3))-g_k), x_k.T)\n",
    "        W = W - step_length * Gradient_MSE\n",
    "        MSEs.append(MSE)\n",
    "    print(f\"MSE in first iteration {MSEs[0]}, and last iteration {MSEs[-1]}\\n\")\n",
    "    print(f\"Our W is \\n{W}\\n\")\n",
    "    return W, MSEs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For deciding the step-length, plotted in the report. Uncomment to see it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSEs_with_diff_step_lengths = []\n",
    "# step_lengths = [0.00001, 0.001, 0.01, 0.09, 0.3, 0.6]\n",
    "# n = 10000\n",
    "# for alpha in step_lengths:\n",
    "#     MSEs_with_diff_step_lengths.append(TrainClassifier(IRIS_Dataset, alpha, n, np.zeros((3, 5)))[1])\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for index ,MSEs in enumerate(MSEs_with_diff_step_lengths):\n",
    "#     plt.plot(MSEs, label=f'Î± = {str(step_lengths[index])}')\n",
    "# plt.legend(fontsize=12)\n",
    "# plt.title(\"MSE for different step lengths\", fontsize=20)\n",
    "# plt.xlabel('Iterations', fontsize=15)\n",
    "# plt.ylabel('Mean Square Error', fontsize=15)\n",
    "# plt.savefig(f\"svg_figures/MSEvsAlpha.svg\")\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters for the best classifier See the one above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "n = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_0 = np.zeros((3, 5))\n",
    "W, _ = TrainClassifier(IRIS_Dataset, alpha, n, W_0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c) training part, confusion matrix, error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestClassifier(dataset: Dataset, W: np.ndarray):\n",
    "    n_classes = 3\n",
    "    n_training = 30\n",
    "    n_testing = 20\n",
    "    ConfMatrices = {\"training\": np.zeros((n_classes, n_classes)), \"testing\": np.zeros((n_classes, n_classes)), \"fileName\": dataset.DESCR}\n",
    "    errors = {\"training\": 0, \"testing\": 0}\n",
    "    for index in range(n_classes * n_testing):\n",
    "        x_k = IRIS_Dataset.data[\"testing\"][\"features\"][index]\n",
    "        x_k = np.append(x_k, 1)\n",
    "        t_k = IRIS_Dataset.data[\"testing\"][\"targets\"][index]\n",
    "        g_k = np.dot(W, x_k)\n",
    "        ConfMatrices[\"testing\"][np.argmax(t_k), np.argmax(g_k)] += 1\n",
    "        if np.argmax(t_k) != np.argmax(g_k):\n",
    "            errors[\"testing\"] += 1\n",
    "    \n",
    "    for index in range(n_classes * n_training):\n",
    "        x_k = IRIS_Dataset.data[\"training\"][\"features\"][index]\n",
    "        x_k = np.append(x_k, 1)\n",
    "        t_k = IRIS_Dataset.data[\"training\"][\"targets\"][index]\n",
    "        g_k = np.dot(W, x_k)\n",
    "        ConfMatrices[\"training\"][np.argmax(t_k), np.argmax(g_k)] += 1\n",
    "        if np.argmax(t_k) != np.argmax(g_k):\n",
    "            errors[\"training\"] += 1\n",
    "    \n",
    "    print(f\"Error-rates: \\nTraining: {errors['training']/(n_training*n_classes)}, Testing: {errors['testing']/(n_classes*n_testing)}\\n\")\n",
    "    return ConfMatrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotConfusionMatrix(matrix, fileName: str):\n",
    "    plt.figure()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16,8))\n",
    "    \n",
    "    ax[0].set_title(\"Training set\",fontsize=22)\n",
    "    ax[1].set_title(\"Testing set\", fontsize=22)\n",
    "    metrics.ConfusionMatrixDisplay(confusion_matrix=matrix[\"training\"], \n",
    "                                   display_labels=IRIS_Dataset.classes_names,\n",
    "                                   ).plot(ax=ax[0], cmap=\"cividis\")\n",
    "    metrics.ConfusionMatrixDisplay(confusion_matrix=matrix[\"testing\"],\n",
    "                                   display_labels=IRIS_Dataset.classes_names,\n",
    "                                   ).plot(ax=ax[1], cmap=\"cividis\")\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"{fileName}\", fontsize=20, x=0.11)\n",
    "    plt.savefig(f\"svg_figures/{fileName}.svg\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfMatrix = TestClassifier(IRIS_Dataset, W)\n",
    "PlotConfusionMatrix(ConfMatrix, \"Iris_allFeaturesTrainFirst\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Switching ordering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a little bit hard coded. Since we should only use it once\n",
    "path = \"Iris_TTT4275/iris.data\"\n",
    "n_classes = 3\n",
    "n_training = 30\n",
    "n_testing  = 20\n",
    "\n",
    "Instances = []\n",
    "\n",
    "with open(path) as file:\n",
    "    for _ in range(n_classes):\n",
    "        for _ in range(n_testing):\n",
    "            line = file.readline()\n",
    "            line = line.split(',')\n",
    "            features = line[0:-1]\n",
    "            label = line[-1].strip(\"\\n\")\n",
    "            testing_instance = Instance(features=features, label=label, set=\"testing\")\n",
    "            Instances.append(testing_instance)\n",
    "            \n",
    "        for _ in range(n_training):\n",
    "            line = file.readline()\n",
    "            line = line.split(',')\n",
    "            features = line[0:-1]\n",
    "            label = line[-1].strip(\"\\n\")\n",
    "            training_instance = Instance(features=features, label=label, set=\"training\")\n",
    "            Instances.append(training_instance)\n",
    "IRIS_Dataset = Dataset(Instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_0 = np.zeros((3, 5))\n",
    "\n",
    "W = W_0\n",
    "MSEs = []\n",
    "for i in range(n):\n",
    "    Gradient_MSE = np.zeros(W_0.shape)\n",
    "    MSE = 0\n",
    "    for index in range(n_classes * n_training):\n",
    "        x_k = IRIS_Dataset.data[\"training\"][\"features\"][index]\n",
    "        x_k = np.append(x_k, 1)\n",
    "        z_k = np.dot(W, x_k)\n",
    "        g_k = sigmoid(z_k)\n",
    "        t_k = IRIS_Dataset.data[\"training\"][\"targets\"][index]\n",
    "        MSE += 1/2 * np.dot((g_k - t_k).T,(g_k - t_k))\n",
    "        Gradient_MSE += np.outer((g_k - t_k)*g_k*(np.ones((1,3))-g_k), x_k.T)\n",
    "    W = W - alpha * Gradient_MSE\n",
    "    MSEs.append(MSE)\n",
    "print(f\"MSE in first iteration {MSEs[0]}, and last iteration {MSEs[-1]}\\n\")\n",
    "print(f\"Our W is {W}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfMatrix = {\"training\": np.zeros((n_classes, n_classes)), \"testing\": np.zeros((n_classes, n_classes)), \"fileName\": IRIS_Dataset.DESCR}\n",
    "errors = {\"training\": 0, \"testing\": 0}\n",
    "for index in range(n_classes * n_testing):\n",
    "    x_k = IRIS_Dataset.data[\"testing\"][\"features\"][index]\n",
    "    x_k = np.append(x_k, 1)\n",
    "    t_k = IRIS_Dataset.data[\"testing\"][\"targets\"][index]\n",
    "    g_k = np.dot(W, x_k)\n",
    "    ConfMatrix[\"testing\"][np.argmax(t_k), np.argmax(g_k)] += 1\n",
    "    if np.argmax(t_k) != np.argmax(g_k):\n",
    "        errors[\"testing\"] += 1\n",
    "\n",
    "for index in range(n_classes * n_training):\n",
    "    x_k = IRIS_Dataset.data[\"training\"][\"features\"][index]\n",
    "    x_k = np.append(x_k, 1)\n",
    "    t_k = IRIS_Dataset.data[\"training\"][\"targets\"][index]\n",
    "    g_k = np.dot(W, x_k)\n",
    "    ConfMatrix[\"training\"][np.argmax(t_k), np.argmax(g_k)] += 1\n",
    "    if np.argmax(t_k) != np.argmax(g_k):\n",
    "        errors[\"training\"] += 1\n",
    "print(f\"Error-rates: \\nTraining: {errors['training']/(n_training*n_classes)}, Testing: {errors['testing']/(n_classes*n_testing)}\\n\")\n",
    "PlotConfusionMatrix(ConfMatrix, \"Iris_allFeaturesTrainLast\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a) Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data \n",
    "n_examples = 50\n",
    "paths = [\"Iris_TTT4275/class_1\", \"Iris_TTT4275/class_2\", \"Iris_TTT4275/class_3\"]\n",
    "features = {paths[0]: [[], [], [], []], paths[1]:[[], [], [], []], paths[2]:[[], [], [], []]}\n",
    "for i in range(len(paths)):\n",
    "    with open(paths[i]) as file:\n",
    "        for j in range(n_examples):\n",
    "            line = file.readline().strip('\\n').split(',')\n",
    "            for k in range(len(line)):\n",
    "                features[paths[i]][k].append(float(line[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying data\n",
    "n_bars = 6\n",
    "opacity = 0.4\n",
    "plt.figure(figsize=(13,13))\n",
    "plt.suptitle(\"Histograms of Iris dataset features\", fontsize=28)\n",
    "for index, feature in enumerate(IRIS_Dataset.feature_names):\n",
    "    plt.subplot(2, 2, index+1)\n",
    "    for class_nr in range(n_classes):\n",
    "        plt.title(f\"feature #{index+1}, {feature[:-5]}\", fontsize=18)\n",
    "        plt.hist(features[paths[class_nr]][index], label=IRIS_Dataset.classes_names[class_nr], bins=n_bars, alpha=opacity, edgecolor='black', color=IRIS_Dataset.colors[class_nr])\n",
    "    plt.legend()\n",
    "    plt.xlabel(feature, fontsize=18)\n",
    "    plt.ylabel('count  [1]',fontsize=18)\n",
    "    plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"svg_figures/Iris_histograms.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Can be seen by the plots, feature 'Sepal width' is caotic and should be excluded. This is done below. Afterwards we scratch sepal length, the petal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS_Dataset = loadDataSet([0, 2, 3])\n",
    "W_0 = np.zeros((3, 4))\n",
    "W, _ = TrainClassifier(IRIS_Dataset, alpha, n, W_0)\n",
    "ConfMatrix = TestClassifier(IRIS_Dataset, W)\n",
    "PlotConfusionMatrix(ConfMatrix, \"ThreeFeatures\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with two features Petal width & length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS_Dataset = loadDataSet([2, 3])\n",
    "W_0 = np.zeros((3, 3))\n",
    "W, _ = TrainClassifier(IRIS_Dataset, alpha, n, W_0)\n",
    "ConfMatrix = TestClassifier(IRIS_Dataset, W)\n",
    "PlotConfusionMatrix(ConfMatrix, \"TwoFeatures\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And with only one feature (Petal width):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS_Dataset = loadDataSet([3])\n",
    "W_0 = np.zeros((3, 2))\n",
    "W, _ = TrainClassifier(IRIS_Dataset, alpha, n, W_0)\n",
    "ConfMatrix = TestClassifier(IRIS_Dataset, W)\n",
    "PlotConfusionMatrix(ConfMatrix, \"OneFeature\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making of scatter plot, also commented out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_classes = 3\n",
    "# plt.figure(figsize=(13,13))\n",
    "# for index in range(n_classes):\n",
    "#     plt.scatter(features[paths[index]][2], features[paths[index]][3], label=IRIS_Dataset.classes_names[index], color=IRIS_Dataset.colors[index], s=200)\n",
    "# plt.legend(fontsize=20)\n",
    "# plt.grid()\n",
    "# plt.xlabel(\"Feature #3, Petal length [cm]\", fontsize=20)\n",
    "# plt.ylabel(\"Feature #4 Petal width [cm]\", fontsize=20)\n",
    "# plt.title(\"Scatter plot of Iris dataset\", fontsize=30)\n",
    "# plt.savefig(\"svg_figures/Iris_scatter.svg\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
