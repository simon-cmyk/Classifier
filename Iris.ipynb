{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for the implementation of task IRIS\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Importing labeled data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, instances):\n",
    "        data = {\"training\": {\"targets\": [], \"features\":[]}, \"testing\": {\"targets\": [], \"features\":[]}}\n",
    "        labelToTarget = {\"Iris-setosa\": [1, 0, 0], \"Iris-versicolor\": [0, 1, 0], \"Iris-virginica\": [0, 0, 1]}\n",
    "        for instance in instances:\n",
    "            match instance.set:\n",
    "                case 'training':\n",
    "                    data[\"training\"][\"targets\"].append(labelToTarget[instance.label])\n",
    "                    data[\"training\"][\"features\"].append(instance.features)\n",
    "                case 'testing':\n",
    "                    data[\"testing\"][\"targets\"].append(labelToTarget[instance.label])\n",
    "                    data[\"testing\"][\"features\"].append(instance.features)\n",
    "                    \n",
    "        # convert to numpy array\n",
    "        data[\"training\"][\"targets\"] = np.array(data[\"training\"][\"targets\"]).astype(float)\n",
    "        data[\"training\"][\"features\"] = np.array(data[\"training\"][\"features\"]).astype(float)\n",
    "        data[\"testing\"][\"targets\"] = np.array(data[\"testing\"][\"targets\"]).astype(float)\n",
    "        data[\"testing\"][\"features\"]  = np.array(data[\"testing\"][\"features\"]).astype(float)\n",
    "        \n",
    "        self.data = data\n",
    "        self.feature_names = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "        self.DESCR = \"Iris dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Instance:\n",
    "    def __init__(self, features, label, set):\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        self.set = set    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Iris_TTT4275/iris.data\"\n",
    "n_classes = 3\n",
    "n_examples = 50\n",
    "n_training = 30\n",
    "n_testing  = 20\n",
    "\n",
    "Instances = []\n",
    "\n",
    "with open(path) as file:\n",
    "    for _ in range(n_classes):\n",
    "        for _ in range(n_training):\n",
    "            line = file.readline()\n",
    "            line = line.split(',')\n",
    "            features = line[0:-1]\n",
    "            label = line[-1].strip(\"\\n\")\n",
    "            training_instance = Instance(features=features, label=label, set=\"training\")\n",
    "            Instances.append(training_instance)\n",
    "        for _ in range(n_testing):\n",
    "            line = file.readline()\n",
    "            line = line.split(',')\n",
    "            features = line[0:-1]\n",
    "            label = line[-1].strip(\"\\n\")\n",
    "            training_instance = Instance(features=features, label=label, set=\"testing\")\n",
    "            Instances.append(training_instance)\n",
    "IRIS_Dataset = Dataset(Instances)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Training a linear classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear classifier equations\n",
    "\n",
    "from the [Compendium-Part-III-Classification](Resources/Compendium_Classification.pdf)\n",
    "\n",
    "##### Chapter 2.4 \n",
    "\n",
    "$$\n",
    "x \\in \\omega_j \\iff g_j(x) = \\max_i g_i(x)\n",
    "$$\n",
    "x is in the class ($\\omega_j$) that has the highest value for the discriminant function ($g_j$). This is equivalent to making a partition of the whole input space, and for each point you assign the point to the class with the highest \"probability\" that the point belongs to that class.\n",
    "The discrimination function:\n",
    "$$\n",
    "g_i(x) = \\omega_i^T x + \\omega_{io} , \\qquad i = i, \\dots, C \n",
    "$$\n",
    "Where $\\omega_i$ is the class $x$ is the feature vector, and the $\\omega_{io}$ is the offset for the class i. In our case C =3, so we can write the disctimination function in compact form: \n",
    "$$\n",
    "g = W x + \\omega_o \n",
    "$$\n",
    "where $W \\in M_{CxF}(\\mathbb{R})$ and $g$ and $\\omega_o$ both are column vectors with size C (#classes). F stands for #features. In our case:  $C=3 \\land F=4$    \n",
    "When training we are aiming at finding the best values for $W$ and $\\omega_o$\n",
    "\n",
    "##### Chapter 3.2 \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get only one linear term, we do one more trick:\n",
    "This trick yields the same equations only that it now gets more compact once again\n",
    "$$\n",
    "\\begin{bmatrix} W & \\omega_o \\end{bmatrix} \\to W\\\\\n",
    "\\begin{bmatrix} x^T & 1 \\end{bmatrix} \\to x \\\\\n",
    "\\implies g = Wx\n",
    "$$\n",
    "Loss function\n",
    "$$\n",
    "MSE = \\frac{1}{2} \\sum_{k=1}^{N}(g_k-t_k)^T(g_k-t_k)\n",
    "$$\n",
    "To decide how good our model is doing. Here t is the target vector (correct labeled features for the training set.)\n",
    "Activation function:\n",
    "$$\n",
    "g_{k} = \\frac{1}{1+\\exp^{-z_{ik}}}, \\qquad z_{ik} = Wx_k \\\\\n",
    "$$\n",
    "We should ideally use a heavyside function, but we need the derivative, and therefor we us the function up above (sigmoid function). This is used for the discrimination function.\n",
    "\n",
    "The way to go? We want to minimize the MSE, based on choosing the W matrix. We use gradient descent.\n",
    "Then we need to hyperparameters which we are tuning:\n",
    "$$\n",
    "\\alpha: \\text{Learning rate [0.0-1.0]}\\\\\n",
    "n: \\text{number of epochs (how many times we improve the W-matrix [0<])}\n",
    "$$\n",
    "Based on them we are using the gradient descent:\n",
    "$$\n",
    "\\text{for i in range(1, n+1):}\\\\\n",
    " \\qquad  \\qquad W_{i+1} = W_{i} - \\alpha \\nabla_WMSE\n",
    "$$\n",
    "For the calculation of the gradient we once again have calculations from the [Compendium-Part-III-Classification](Resources/Compendium_Classification.pdf)\n",
    "\n",
    "Using the chain rule\n",
    "$$\n",
    "\\nabla_W MSE = \\sum^{N}_{k=1}\\nabla_{g_{k}} \\, MSE \\, \\nabla_{z_{k}} \\, g_k \\, \\nabla_W z_k\\\\\n",
    "$$\n",
    "Some simplifications\n",
    "$$\n",
    "\\nabla_{g_k}MSE = g_k-t_k\\\\\n",
    "\\nabla_{z_k}g = g_k \\circ (1-g_k)\\\\\n",
    "\\nabla_Wz_k = x_k^T\\\\\n",
    "$$\n",
    "Yields the result\n",
    "$$\n",
    "\\nabla_W MSE = \\sum^{N}_{k=1} [(g_k-t_k) \\circ g_k \\circ (1-g_k)]x_k^T\n",
    "$$\n",
    "Where $\\circ$ is elementwise multiplication."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hyperparameters, step length and number of epochs\n",
    "'''\n",
    "alpha = 0.1\n",
    "n = 1000\n",
    "W_0 = np.random.randn(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our choice of W:\n",
      " [[-37.14066229 -17.10293466 -28.72196203  -9.47165472]\n",
      " [ -2.32917669  -1.31729313  -1.31383239  -0.17798326]\n",
      " [-27.81393971 -15.87039851 -15.05978345  -4.42628152]]\n",
      "MSE in first iteration 54.27535737593905, and last iteration 44.99999998615936\n"
     ]
    }
   ],
   "source": [
    "W = W_0\n",
    "MSEs = []\n",
    "for i in range(n):\n",
    "    Gradient_MSE = np.zeros((3, 4))\n",
    "    MSE = 0\n",
    "    for index in range(n_classes * n_training):\n",
    "        x_k = IRIS_Dataset.data[\"training\"][\"features\"][index]\n",
    "        z_k = np.dot(W, x_k)\n",
    "        g_k = sigmoid(z_k)\n",
    "        t_k = IRIS_Dataset.data[\"training\"][\"targets\"][index]\n",
    "        MSE += 1/2 * np.dot((g_k - t_k).T,(g_k - t_k))\n",
    "        \n",
    "        Gradient_MSE += np.outer(np.multiply(g_k - t_k, g_k, np.ones((1, 3))-g_k), x_k.T)\n",
    "        \n",
    "    W = W - alpha * Gradient_MSE    \n",
    "    MSEs.append(MSE)\n",
    "print(f\"Our choice of W:\\n {W}\")\n",
    "print(f\"MSE in first iteration {MSEs[0]}, and last iteration {MSEs[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 0\n",
      "Wrongly labelled instance found\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n",
      "1 2\n",
      "Wrongly labelled instance found\n"
     ]
    }
   ],
   "source": [
    "for index in range(n_classes * n_training):\n",
    "    x_k = IRIS_Dataset.data[\"training\"][\"features\"][index]\n",
    "    t_k = IRIS_Dataset.data[\"training\"][\"targets\"][index]\n",
    "    g_k = np.dot(W, x_k)\n",
    "    print(np.argmax(g_k), np.argmax(t_k))\n",
    "    if np.argmax(g_k) != np.argmax(t_k):\n",
    "        print(\"Wrongly labelled instance found\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n",
      "Wrongly labelled instance found\n"
     ]
    }
   ],
   "source": [
    "for index in range(n_classes * n_testing):\n",
    "    x_k = IRIS_Dataset.data[\"testing\"][\"features\"][index]\n",
    "    t_k = IRIS_Dataset.data[\"testing\"][\"targets\"][index]\n",
    "    g_k = np.dot(W, x_k)\n",
    "    if np.argmax(g_k) != np.argmax(t_k):\n",
    "        print(\"Wrongly labelled instance found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
