{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for the implementation of task MNIST"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "from keras.datasets import mnist\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a) Creating NN classifier with chunksize 1000 and plot confusion matrix and print error rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading of data and declaring classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, train_X, train_y, test_X, test_y):\n",
    "        data = {\"training\": {\"targets\": [], \"features\":[]}, \"testing\": {\"targets\": [], \"features\":[]}}\n",
    "                    \n",
    "        data[\"training\"][\"features\"] = train_X\n",
    "        data[\"training\"][\"targets\"] = train_y\n",
    "        data[\"testing\"][\"features\"] = test_X\n",
    "        data[\"testing\"][\"targets\"]  = test_y\n",
    "        \n",
    "        self.data = data\n",
    "        self.DESCR = \"MNISK dataset\"\n",
    "        self.n_training = 60000\n",
    "        self.n_testing = 10000\n",
    "        self.chunk_size = 1000\n",
    "        self.n_chunks = self.n_training//self.chunk_size\n",
    "        self.classes_names = ['0','1','2','3','4','5','6','7','8','9']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the MNIST dataset\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "# reshape to fit for our class.\n",
    "train_X = np.reshape(train_X, (train_X.shape[0], train_X.shape[1] * train_X.shape[2]))\n",
    "test_X = np.reshape(test_X, (test_X.shape[0], test_X.shape[1] * test_X.shape[2]))\n",
    "MNIST_Dataset = Dataset(train_X, train_y, test_X, test_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_predict(images_predict, n_chunks, chunk_size, train_set):\n",
    "    predicted = []\n",
    "    time_s = time.time()\n",
    "    for image in images_predict:\n",
    "        min_dict = {\"indices\": [], \"values\": []}\n",
    "        for i in range(n_chunks):\n",
    "            # Divide into chunks\n",
    "            reduced_test_set = train_set[\"features\"][i*chunk_size:(i+1)*chunk_size]\n",
    "            distances = np.ravel(distance.cdist([image], reduced_test_set, 'euclidean'))\n",
    "            # find the minimum euclidean distance from each chunk\n",
    "            min_cluster_index = np.argmin(distances)\n",
    "            min_global_index = i*chunk_size+min_cluster_index\n",
    "            min_dict[\"indices\"].append(min_global_index)\n",
    "            min_dict[\"values\"].append(distances[min_cluster_index])\n",
    "        # find the minimum euclidean distance from all chunks\n",
    "        min_dict_NN_index = np.argmin(min_dict[\"values\"])\n",
    "        NN_global_index = min_dict[\"indices\"][min_dict_NN_index]\n",
    "        predicted.append(train_set[\"targets\"][NN_global_index])\n",
    "    print(f\"time for Predicting: {time.time()-time_s}\")\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotConfusionMatrix(matrix, fileName: str):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    metrics.ConfusionMatrixDisplay(confusion_matrix=matrix, \n",
    "                                   display_labels=MNIST_Dataset.classes_names,\n",
    "                                   ).plot(cmap=\"viridis\", ax=ax)\n",
    "    plt.title(\"NN classifier\", fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"{fileName}\", fontsize=18, x=0.1)\n",
    "    plt.savefig(f\"svg_figures/{fileName}.svg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix_test(MNIST_Dataset: Dataset):\n",
    "    n_classes = 10\n",
    "    data = MNIST_Dataset.data\n",
    "    testing_data = data[\"testing\"][\"features\"]\n",
    "    # Compute the prediction\n",
    "    prediction = NN_predict(testing_data, MNIST_Dataset.n_chunks, MNIST_Dataset.chunk_size, data[\"training\"])\n",
    "    # For statistics\n",
    "    conf_matrix = np.zeros((n_classes, n_classes)).astype(int)\n",
    "    n_misclassified = 0\n",
    "    misclassified_examples = []\n",
    "    correctly_classified_examples = []\n",
    "    for i in range(len(prediction)):\n",
    "        target = data[\"testing\"][\"targets\"][i]\n",
    "        pred = prediction[i]\n",
    "        conf_matrix[target, pred] += 1 \n",
    "        if target != pred and len(misclassified_examples) < 4:\n",
    "            n_misclassified += 1\n",
    "            misclassified_examples.append([testing_data[i], pred, target])\n",
    "        elif target != pred:\n",
    "            n_misclassified += 1 \n",
    "        elif target == pred and len(correctly_classified_examples) < 4:\n",
    "            correctly_classified_examples.append([testing_data[i], pred, target])\n",
    "\n",
    "    error_rate = float(n_misclassified) / len(testing_data)\n",
    "    return conf_matrix, error_rate, misclassified_examples, correctly_classified_examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix, error_rate, misclassified_examples, correctly_classified_examples = conf_matrix_test(MNIST_Dataset)\n",
    "print(\"Error rate: \" + str(error_rate))\n",
    "PlotConfusionMatrix(confusion_matrix, \"NN_test_set\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b, c) Plot some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotExamples(examples, fileName: str):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "    for i, (image, predicted, target) in enumerate(examples):\n",
    "        ax = axs[i//2, i%2]\n",
    "        ax.set_title(f\"Classified as {predicted}, target is {target}\", fontsize=18)\n",
    "        ax.imshow(np.reshape(image, (28, 28)), cmap='cividis')\n",
    "\n",
    "    fig.subplots_adjust(hspace=0.4)\n",
    "    plt.savefig(f\"svg_figures/{fileName}.svg\")\n",
    "    # using svg so that the graphics looks good in latex\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotExamples(misclassified_examples, \"misclassified\")\n",
    "PlotExamples(correctly_classified_examples, \"Correctly_classified\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a) Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateClusters(n_clusters, train_x, train_y):\n",
    "    time_s = time.time()\n",
    "    # sorting the data\n",
    "    n_classes = 10\n",
    "    tuples = [(train_x[i], train_y[i]) for i in range(len(train_x))]\n",
    "    tuples = sorted(tuples, key=lambda x: x[1])\n",
    "    train_x = np.array([t[0] for t in tuples])\n",
    "    train_y = np.array([t[1] for t in tuples])\n",
    "    # flatten (normalize)\n",
    "    train_x = train_x.flatten().reshape(train_x.shape)\n",
    "    # using kmeans to create clusters \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    # trick to find the break points for when the classes change\n",
    "    break_points = [0]\n",
    "    for i in range(len(train_y)-1):\n",
    "        if train_y[i] != train_y[i+1]:\n",
    "            break_points.append(i+1)\n",
    "    break_points.append(len(train_x)-1)\n",
    "    \n",
    "    clusters = np.empty((n_classes, n_clusters, train_x.shape[1]))\n",
    "    for i in range(n_classes):\n",
    "        clusters[i] = kmeans.fit(train_x[break_points[i]:break_points[i+1]]).cluster_centers_\n",
    "    # clusters have all of the cluster centers for each class (64 each)\n",
    "    clusters = clusters.flatten().reshape(n_classes*n_clusters, train_x.shape[1])\n",
    "    print(f\"time for Clustering: {time.time()-time_s}\")\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_test(clusters, test_x, test_y):\n",
    "    n_classes = 10\n",
    "    test_x = test_x.flatten().reshape(test_x.shape)\n",
    "    # for saving statistics\n",
    "    conf_matrix = np.zeros((n_classes, n_classes)).astype(int)\n",
    "    errors = 0\n",
    "    time_s2 = time.time()\n",
    "    for index, img in enumerate(test_x):\n",
    "        distances = []\n",
    "        for cluster in clusters:\n",
    "             distances.append(distance.euclidean(img, cluster))\n",
    "        # Divide index by 64 to get the class (64 cluster per class)\n",
    "        pred = np.argmin(distances) // 64\n",
    "        conf_matrix[test_y[index], pred] += 1\n",
    "        if pred != test_y[index]:\n",
    "            errors += 1\n",
    "    print(f\"time for Predicting: {time.time()-time_s2}\")\n",
    "    print(\"Error rate: \" + str(errors/len(test_y)))\n",
    "    PlotConfusionMatrix(conf_matrix, \"cNN_test_set\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = CreateClusters(64, MNIST_Dataset.data[\"training\"][\"features\"], MNIST_Dataset.data[\"training\"][\"targets\"])  \n",
    "cnn_test(clusters, MNIST_Dataset.data[\"testing\"][\"features\"], MNIST_Dataset.data[\"testing\"][\"targets\"])  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is a little bit poorer, but the time usage is quite substantially better"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c) Designing a KNN classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cknn_test(clusters, k, test_x, test_y):\n",
    "    n_classes = 10\n",
    "    test_x = test_x.flatten().reshape(test_x.shape)\n",
    "    conf_matrix = np.zeros((n_classes, n_classes)).astype(int)\n",
    "    errors = 0\n",
    "    time_s2 = time.time()\n",
    "    for index, img in enumerate(test_x):\n",
    "        distances = []\n",
    "        for cluster in clusters:\n",
    "             distances.append(distance.euclidean(img, cluster))\n",
    "        # Basically same as the last one but with k neighbors. Therefore we\n",
    "        # need a different rule for voting\n",
    "        top_k = np.argsort(distances)[:k]\n",
    "        votes = [0] * n_classes\n",
    "        for val in top_k:\n",
    "            votes[(val // 64)] += 1\n",
    "        pred = np.argmax(votes)\n",
    "        conf_matrix[test_y[index], pred] += 1\n",
    "        if pred != test_y[index]:\n",
    "            errors += 1\n",
    "    print(f\"time for Predicting: {time.time()-time_s2}\")\n",
    "    print(\"Error rate: \" + str(errors/len(test_y)))\n",
    "    PlotConfusionMatrix(conf_matrix, \"cKNN_test_set\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cknn_test(clusters, 7, MNIST_Dataset.data[\"testing\"][\"features\"], MNIST_Dataset.data[\"testing\"][\"targets\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again it is a little bit poorer than the NN, but takes a lot less time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
